#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{amsmath}
\usepackage{bm}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
% set default figure placement to htbp
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Homework 3 - Blackjack
\end_layout

\begin_layout Author
Linyang He
\end_layout

\begin_layout Section*
Problem 1: Value Iteration
\end_layout

\begin_layout Subsection*
a.
 Give the value
\end_layout

\begin_layout Standard
We should notice that the first iteration just initializes all the values.
 Besides, the terminal states should not accept any new iteration.
 So the value of -2 or 2 are always 0.
\end_layout

\begin_layout Standard
Iteration 0:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$V(-2) = V(-1) = V(0) = V(1) = V(2) = 0$$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Iteration 1:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$V_{opt}(-2) = V_{opt}(2)= 0$$ 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$V_{opt}(-1) = max
\backslash
{ 0.8(20 + 0) + 0.2(-5 + 0), 0.3(-5 + 0) + 0.7(20 + 0)
\backslash
} = 15$$ 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$V_{opt}(0) = max
\backslash
{ 0.8(-5 + 0) + 0.2(-5 + 0), 0.3(-5 + 0) + 0.7(-5 + 0)
\backslash
} = -5$$ 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$V_{opt}(1) = max
\backslash
{ 0.8(-5 + 0) + 0.2(100 + 0), 0.3(100 + 0) + 0.7(-5 + 0)
\backslash
} = 26.5$$ 
\end_layout

\end_inset

So when it comes to the third iteration, we should notice that all the new
 optimal value in the iteration 1 should be used.
\end_layout

\begin_layout Standard
Iteration 2:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$V_{opt}(-2) = V_{opt}(2)= 0$$ 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$V_{opt}(-1) = max
\backslash
{ 0.8(20 + 0) + 0.2(-5 -5), 0.3(-5-5) + 0.7(20 + 0)
\backslash
} = 14$$ 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$V_{opt}(0) = max
\backslash
{ 0.8(-5 + 15) + 0.2(-5 + 26.5), 0.3(-5 + 26.5) + 0.7(-5 + 15)
\backslash
} = 13.45$$ 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$V_{opt}(1) = max
\backslash
{ 0.8(-5 -5) + 0.2(100 + 0), 0.3(100 + 0) + 0.7(-5 -5)
\backslash
} = 23$$ 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
b.
 What is the resulting optimal policy
\end_layout

\begin_layout Standard
Considering the computation progress in the above question, we have:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$
\backslash
pi_{opt}(-2) = 
\backslash
pi_{opt}(2) = end state$$
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$
\backslash
pi_{opt}(-1) = s-1$$ 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$
\backslash
pi_{opt}(0) = s+1$$ 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$$
\backslash
pi_{opt}(1) = s+1$$
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Problem 2: Transforming MDPs
\end_layout

\begin_layout Enumerate
The 
\begin_inset ERT
status open

\begin_layout Plain Layout

$V_1(s_{start})$
\end_layout

\end_inset

 is not always equal to or greater than 
\begin_inset ERT
status open

\begin_layout Plain Layout

$V_2(s_{start})$
\end_layout

\end_inset

.
 You can find the counter example in the submission.py.
\end_layout

\begin_layout Enumerate
Considering that the MDP is acyclic, the markov chain just goes forward
 and never goes back.
 So if we want to get the optimal global reward, we just need to get the
 optimal reward for each recurrence.
 Therefore, we can use dynamic programming algorithm to compute the V(s),
 then the algorihm goes each (s, a, s') triple just once time.
\end_layout

\begin_layout Enumerate
Just consider state 
\emph on
o 
\emph default
as a new state that each state can lead to.
 In other word, we add another option for each iteration as the following
 figure shows.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename pasted1.png
	scale 60

\end_inset


\end_layout

\begin_layout Standard
We can get have new 
\emph on
T'(s,a,s') 
\emph default
and new 
\emph on
Reward'(s,a,s').
 
\emph default
We have:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$
\end_layout

\begin_layout Plain Layout

T'(s,a,s') = 
\backslash
left
\backslash
{
\end_layout

\begin_layout Plain Layout

        
\backslash
begin{array}{ll}
\end_layout

\begin_layout Plain Layout

            
\backslash
gamma T(s,a,s')&
\backslash
quad s' 
\backslash
in States
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

            1- 
\backslash
gamma & 
\backslash
quad s' = o
\end_layout

\begin_layout Plain Layout

        
\backslash
end{array}
\end_layout

\begin_layout Plain Layout

    
\backslash
right
\end_layout

\begin_layout Plain Layout

$$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
and
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$
\end_layout

\begin_layout Plain Layout

Reward'(s,a,s') = 
\backslash
left
\backslash
{
\end_layout

\begin_layout Plain Layout

        
\backslash
begin{array}{ll}
\end_layout

\begin_layout Plain Layout

            
\backslash
dfrac{1}{
\backslash
gamma} Reward(s,a,s')&
\backslash
quad s' 
\backslash
in States
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

           0 & 
\backslash
quad s' = o
\end_layout

\begin_layout Plain Layout

        
\backslash
end{array}
\end_layout

\begin_layout Plain Layout

    
\backslash
right
\end_layout

\begin_layout Plain Layout

$$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Then we can get our new 
\begin_inset ERT
status open

\begin_layout Plain Layout

$V_{opt}'(s)$
\end_layout

\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$
\end_layout

\begin_layout Plain Layout


\backslash
begin{align}
\end_layout

\begin_layout Plain Layout

V'_{opt}(s) &= max_{a} 
\backslash
sum_{s' 
\backslash
in States} 
\backslash
gamma T(s,a,s')
\backslash
{
\backslash
dfrac{1}{
\backslash
gamma} Reward(s,a,s')+V'_{opt}(s')
\backslash
} + (1-
\backslash
gamma) 
\backslash
times 0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

	&= max_{a} 
\backslash
sum_{s' 
\backslash
in States} T(s,a,s')
\backslash
{Reward(s,a,s')+
\backslash
gamma V'_{opt}(s')
\backslash
}
\end_layout

\begin_layout Plain Layout


\backslash
end{align}
\end_layout

\begin_layout Plain Layout

$$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Consider the recurrence, we can get: 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$V_{opt}'(s) = V_{opt}(s).$$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
That is, the new MDP has the same optimal values as the original one.
\end_layout

\begin_layout Section*
Rroblem 3: Peeking Blackjack
\end_layout

\begin_layout Subsection*
a.
 Implement the game of Blackjack
\end_layout

\begin_layout Standard
According to the description in the document, we implement the Blackjack
 game in submission.py.
 We build the framework based on the actions.
\end_layout

\begin_layout Subsection*
b.
 Build a peekingMDP
\end_layout

\begin_layout Standard
Consider that at least 10% of states should be peeking, it means that it's
 safer at least in 10% chance that you peek the top card.
 Just build a situation where the more you peek, the safer you are.
 We just put a quite large value card, which can lead the players to peek
 more.
 You can more details in the python script file.
\end_layout

\begin_layout Section*
Problem 4: Learning to Play Blackjack
\end_layout

\begin_layout Subsection*
a.
 Q-Learning Algorithm for Blackjack
\end_layout

\begin_layout Standard
The key of the Q-Learning algorithm is as following.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename pasted2.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
In our training processing, we will use the Bellman Equation to update the
 Q-Table:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$$Q(s,a) = r + 
\backslash
gamma(max(Q(s',a')))$$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We will build our code based on this equation.
 You can find more details in the python script.
\end_layout

\begin_layout Subsection*
b.
 Compare
\end_layout

\begin_layout Standard
We build a function named problem_4b to analysis this problem.
 Compare the policy learned in this case to the policy learned by value
 iteration.
 We can have the results as follows:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Scale
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
smallMDP
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
largeMDP
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Total Actions
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
27
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2745
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Different Actions
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1017
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Different Rate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.7%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
37%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
According to the table, we can find that Q-Learning performs bettern in
 smallMDP.
 We believe this is because the state space is relatively small.
 When the state space is small, the environment allows the Q-Learning learn
 the Q values for (state, action) pair better.
 
\end_layout

\begin_layout Standard
However, the Q-Learning algorithm works poor in the largeMDP task.
 Considering that identityFeatureExtractor actually can not describe unseen
 states' values, the Q-learning isn't able to accurately compute the (state,
 action) quite much.
\end_layout

\begin_layout Subsection*
c.
 Build Blackjack Feature Extractor
\end_layout

\begin_layout Standard
We implement the following features: 
\end_layout

\begin_layout Itemize
Indicator on the total and the action (1 feature).
 
\end_layout

\begin_layout Itemize
Indicator on the presence/absence of each card and the action (1 feature).
 # Example: if the deck is (3, 4, 0 , 2), then the indicator on the presence
 of each card is (1,1,0,1) 
\end_layout

\begin_layout Itemize
Indicator on the number of cards for each card type and the action (len(counts)
 features)
\end_layout

\begin_layout Standard
More details can find in the python file.
\end_layout

\begin_layout Subsection*
d.
 Compare Fixed RL Algorithm with Q-Learning
\end_layout

\begin_layout Standard
We build a function named problem_4d to analysis this problem.
 After 30000 trails, we can get the result as follows:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Value Iteration
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Q-Learning
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Reward
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6.83
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
9.08
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 We can find that the Value Iteration gets a relatively low reward score.
 This is because that the newThresholdMDP can be used by the Q-Learning
 while can not for the Fixed RL Algorithm.
\end_layout

\end_body
\end_document
